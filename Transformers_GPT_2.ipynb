{
   "cells": [
    
     {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterS111/GPT_2_CODE/blob/main/Open_AI_GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAfruuXn1RJE"
   },
   "source": [
    "\n",
    "# Transformers_GPT_2\n",
    "version 25.05.2022\n",
    "\n",
    "The code is based on https://github.com/priya-dwivedi/Deep-Learning/ by Priyanka Dwivedi, with small changes by Peter S\n",
    "\n",
    "This notebook contains the code which was used in our paper \"Training GPT-2 to represent two Romantic-era authors challenges, evaluations and pitfalls\" to fine-tune and generate text from OpenAI GPT-2 models.\n",
    "\n",
    "Base GPUs on Colab only allow for fine-tuning of the Small (124M) and Medium (345M) models. For the Large model (774M) you will need T4 or P100 GPU.\n",
    "\n",
    "Make sure that you are using a GPU: Runtime/Change runtime type/ -> Select \"GPU\".\n",
    "\n",
    "First, check that your runtime is using a GPU: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfnKQwQQ1_gk"
   },
   "outputs": [],
   "source": [
    "# check the GPU:\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98IxMgje2HZT"
   },
   "source": [
    "## 2. Download the repository from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3Qv6nfmrXPr"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/PeterS111/Transformers_GPT_2_fine_tune_and_generate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLC20YbW2NcD"
   },
   "source": [
    "## 3. Change the working directory to the main folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_xfH-FJrXne"
   },
   "outputs": [],
   "source": [
    "cd /content/Transformers_GPT_2_fine_tune_and_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWcBZeOR2jsb"
   },
   "source": [
    "## 4. Install Tranformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07S832Y0rX1y"
   },
   "outputs": [],
   "source": [
    "pip install \"transformers==2.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DL6au0g7F_v"
   },
   "source": [
    "## 5. Batch fine-tune models and generate samples\n",
    "To run batch fine-tuning and generation go to \"run_all.py\" and edit accordingly. Uncomment the line below and run it. Please be aware that running that script with its original settings will exceed the maximum running time that Colab allows.\n",
    "\n",
    "If you want to experiment with a single model and smaller number of samples, please follow the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPO-b7l97Pue"
   },
   "outputs": [],
   "source": [
    "!python run_all.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLd4egEA2iUz"
   },
   "source": [
    "# 5. Training the model without validation dataset:\n",
    "\n",
    "You can train the model with or without the validation dataset.\n",
    "Datasets of Tolstoy's novels and Shelley's are provided with the notebook. You can of course replace those datasets with your own. In that case you will have to upload them to /content/GPT-2_DEMO_FINE_TUNING/input_data. You can \"drag and drop\" them from your PC. You will have to change the argument: \"--train_data_file\" accordingly.\n",
    "\n",
    "To control the training time you can change the number of epochs.\n",
    "\n",
    "To train the Medium model change the following argument:\n",
    "\n",
    "--model_name_or_path=gpt2\n",
    "\n",
    "to:\n",
    "\n",
    "--model_name_or_path=gpt2-medium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryAY2670rYEJ"
   },
   "outputs": [],
   "source": [
    "!python run_lm_finetuning.py --output_dir=output  --model_type=gpt2 --model_name_or_path=gpt2 --do_train  --train_data_file=input_data/Shelley.txt --overwrite_output_dir --block_size=200 --per_gpu_train_batch_size=1 --save_steps 1000 --save_total_limit 1 --num_train_epochs=5 --logging_steps=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIwoiOLH6hEy"
   },
   "source": [
    "# 6. Training the model with validation dataset:\n",
    "\n",
    "The same as above but if you want to used your own datasets you will have to change the \"--eval_data_file\" and \"--train_data_file\" arguments accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riej0YolrYVt"
   },
   "outputs": [],
   "source": [
    "!python run_lm_finetuning.py --output_dir=output  --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=input_data/Shelley_train.txt --do_eval --eval_data_file=input_data/Shelley_val.txt --overwrite_output_dir --block_size=200 --per_gpu_train_batch_size=1 --save_steps 1000 --save_total_limit 1 --num_train_epochs=5 --logging_steps=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpHMqWDV7sQR"
   },
   "source": [
    "# 7. Generate text:\n",
    "\n",
    "First check the folder /GPT-2_DEMO_FINE_TUNING/output for a folder named \"checkpoint- *\". In our case the folder is \"checkpoint-6000\". If you used a different dataset or change the training time, the folder may be different. Make sure that the parameter \"--model_name_or_path output\" has the correct argument. You can change the \"--prompt\" parameter to any continuous string. If you want to generate more samples you can change the \"--num_samples\" parameter. You can the \"--seed\" parameter to any positive integer.\n",
    "\n",
    "Generated text will appear in the output cell and a copy will be saved in the \"GPT-2_DEMO_FINE_TUNING/outputs\" folder.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAbhLuferZKe"
   },
   "outputs": [],
   "source": [
    "!python run_generation.py --model_type gpt2 --temperature 1.0 --top_k 50 --top_p 1.0 --model_name_or_path output/checkpoint-6000 --length 500 --prompt \"The blue sky\" --seed 37 --num_samples 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwCHsizsNDEU"
   },
   "source": [
    "# 8. Exporting the fine-tuned model to Google Drive\n",
    "\n",
    "If you want to save the fine-tuned model for later use, follow these steps:\n",
    "\n",
    "8.1. Zip the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psapBGyyvCVs"
   },
   "outputs": [],
   "source": [
    "!tar -czvf \"my_model_6000_steps.tar.gz\" output/checkpoint-6000/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjhvwImaNoQb"
   },
   "source": [
    "8.2. Mount Google Drive (it will require authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjG5YkiNx1jx"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKCvk7FuNzKC"
   },
   "source": [
    "8.3. Export the compressed model to Drive. This only takes few minutes, but sometimes you may have to run the command twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBjg7yXrPJri"
   },
   "outputs": [],
   "source": [
    "!cp \"my_model_6000_steps.tar.gz\" \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuM51wb7OG47"
   },
   "source": [
    "#9. Importing saved model from Google Drive\n",
    "\n",
    "If you want to return to saved model, do the following:\n",
    "\n",
    "First, run points 2, 3 and 4 (download the repository, change the directory, install Transformers), the same as above. Then:\n",
    "\n",
    "9.1. Create the output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9xw7VNmvDGH"
   },
   "outputs": [],
   "source": [
    "!mkdir output/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPCGLRuoPdkC"
   },
   "source": [
    " 9.2. Mount Google Drive, the same as 8.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v0d02YBPk3q"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOcpDErrPoAD"
   },
   "source": [
    "9.3. Import the previously saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrF-v_SZ232k"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/my_model_6000_steps.tar.gz\" /content/GPT-2_DEMO_FINE_TUNING/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF_kVCptPuGy"
   },
   "source": [
    "9.4. Unpack the model and remove the .tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xyKdKbzvDUT"
   },
   "outputs": [],
   "source": [
    "!tar xf /content/GPT-2_DEMO_FINE_TUNING/output/my_model_6000_steps.tar.gz\n",
    "!rm -v /content/GPT-2_DEMO_FINE_TUNING/output/my_model_6000_steps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4xxxQKjQL0c"
   },
   "source": [
    "9.5. Ready to generate text. The command below is the same as in point 7, check there for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBgJnJB5vDj_"
   },
   "outputs": [],
   "source": [
    "!python run_generation.py --model_type gpt2 --temperature 1.0 --top_k 50 --top_p 1.0 --model_name_or_path output/checkpoint-6000 --length 500 --prompt \"The blue sky\" --seed 37 --num_samples 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformers_GPT_2_fine_tune_and_generate .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
