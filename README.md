# GPT_2_CODE

version 11.06.2022

This repository contains data and code for the paper:

"Training GPT-2 to represent two Romantic-era authors: challenges, evaluations and pitfalls"

by Piotr Sawicki, Marek Grzes, Anna Jordanous, Dan Brown, Max Peeperkorn

ICCC 2022

## Code

**"Open_AI_GPT-2.ipynb"** notebook allows to fine-tune the OpenAI ("Regular") GPT-2 models and generate samples from them. 

**"Transformers_GPT-2.ipynb"** notebook allows to fine-tune the GPT-2 models from Transformers library ("Language Modeling Head - LMH" models) and generate samples from them. 

The notebooks are ready to use. Click on the "Open in Colab" button, and follow the instructions. Thes notebooks also allows to save the fine-tuned models in Google Drive in order to return to them later.

## Remaining code will be added shortly
Peter S
